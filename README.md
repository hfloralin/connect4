# Comparing SARSA, Q-Learning, and Double Q-Learning against Random Agents in Connect Four
Flora Lin, Joshua Yanowitz, Sammi Wang

## Introduction and Problem Space
Connect Four is a two-player adversarial board game in which players take turns placing pieces on a 7-column, 6-row (7×6) board which is suspended vertically, forcing players to place their pieces from the top and causing them to fall down to the lowest row available in each column. The goal of the game is for each player to be the first to form a vertical, horizontal, or diagonal line of four of their own pieces, without allowing their opponent to form a four-piece line first. 
	Connect Four’s action space is extremely limited due to the format of the game. Despite having a 42-tile board, there are only 7 ways to place a piece due to tiles falling from the top to the bottom, limiting the action space to only 7 actions. On the other hand, the state space is very large, with 342 possible permutations of each tile being either unoccupied or occupied by player 1 or player 2’s tile, though many of these permutations are impossible due to gravity. 
	The nature of the game forces players to strike the balance between placing winning tiles and blocking the opponent from placing their own winning tiles. Connect Four is a solved game. If played correctly in a “perfect” game, player one should be able to win the game given they play that specific strategy. We created a random agent as a benchmark and made it go first in order to prevent the reinforcement learning (RL) agents from solving the game early on. 

## State and Observation Space
The board is a classic 7×6 board, with 42 tiles in an empty board. Each tile can be one of three options: empty, filled by player 1, or filled by player 2. Therefore, the state space is a discrete value of 342 possible permutations. However, many variations are impossible due to the limitations introduced by gravity: only the bottom-most tiles can be filled in the next move. 

## Action Space
Actions are the columns with empty spaces in which one can drop their coin, making the actions in this game discrete. Given an empty board of 7 columns, there are only a maximum of 7 discrete actions a player can take. When a column fills up, then there is one less action for the next player, for the remainder of the game. Therefore in an empty board, the possible discrete actions are given as columns: [0, 1, 2, 3, 4, 5, 6].

## Reward Scheme
Rewards are only assigned when the board reaches a terminal state; all actions that do not cause the game to end are assigned a reward of zero. When causing a win, loss, or draw, rewards are assigned thusly: winning moves have a reward of 1, losing moves a reward of -2, and draws a reward of -1. This discourages the agent from playing towards a draw, while still punishing losses more harshly. We also experimented with an alternative reward scheme where winning is rewarded with +100 and losing is penalized with -200, but the differences in performance were only marginal.

## Methodology
We implemented and analyzed the performance results of three RL agents against a random agent: SARSA, Q-Learning (QL), and Double Q-Learning (DQL). The random agent randomly chooses an action as player one to prevent agents from finding the solved solution. We created a general “TDAgent” superclass that the RL agents extend, with functions to select actions, get rewards, take turns, train, and update Q. The select action function allows the agents to select the next action based on an epsilon-greedy policy. The get rewards function returns a reward of 1, -2, or -1 when the game is at the terminal state of a win, a lose, or a draw, respectively. The take turns function defines the state transitions, executes the selected action, and updates the Q-value for the old state and chosen action. The training function trains the agent for given episodes against a random agent and reports the average time steps and percentage of winning with a line graph. The updates Q function is defined differently for each agent.
While the random agent randomly selects an action, the RL agents learn to take better actions in their turns to maximize their chances of winning (maximizing the cumulative reward) against our baseline agent. The SARSA and QL agents are slight variations of each other: using different strategies to update the q-value. SARSA is an on-policy algorithm that updates the q-value with the next action executed, while Q-Learning is an off-policy algorithm that updates the q-value with the predicted best action for the next state. DQL uses two Q-tables to remove any maximization bias in the Q-Learning solution by using one table for estimation and the other for finding the maximized value. In all three agents, we flattened arrays to tuples in order to store them as dictionary keys for Q-tables. 
After experimenting with the hyperparameters and analyzing their impact on the results, we realized that the epsilon value had the greatest impact on performance. Making the epsilon value as small as possible improved results drastically. Similarly, smaller alpha values improved performance, but only marginally. Finally, gamma barely impacted performance and we kept it around 0.9. The best overall results (SARSA: 99.49% win rate, QL: 99.49% win rate, DQL: 98.55%) used an epsilon value of 0.01, an alpha value of 0.1, and a gamma of 0.7. 

## Results
As previously observed, the hyperparameters changed the results drastically and we experimented with the values of each hyperparameter to achieve the best results. Changes in the epsilon value had the greatest impact on the performance and having a minuscule epsilon made the percentages of winning near 100% for all three algorithms. Higher epsilon values (>= 0.1) resulted in poor performance even with sufficient training, and epsilon values larger than 0.5 made it difficult for the algorithm to terminate. Increasing alpha resulted in a decrease in performance, but not by much. We used 0.9 as the discount factor for the majority of our experimentation as a baseline since gamma hardly impacted the performance but ultimately found that 0.7 marginally improved our results. 
We compared the percentage of wins against the random agent and the average steps taken as our metrics. While all three algorithms would reach nearly 100% wins in the solved game, the steps taken (or convergence) would vary. The hyperparameters that yielded the best results for the SARSA algorithm at 4.04 steps taken and a 99.85% win rate, were epsilon = 0.01, alpha = 0.1, and gamma = 0.9. For QL, the best results were 4.07 steps, and a 99.49% win rate with the values epsilon = 0.01, alpha = 0.1, and gamma = 0.7. Finally, the DQL algorithm performed the best with epsilon = 0.01, alpha = 0.2, and gamma = 0.9 for a win rate of 99.59, and 4.06 steps taken. 
The Q-Learning algorithm was the most consistent in its performance, surprisingly. The QL agent outperformed both SARSA and DQL in terms of overall average, with the lowest standard deviation of results across hyperparameters. 

## Discussion
Connect 4 is a solved game for player one, given perfect moves. Our goal initially was to simply beat the random agent to demonstrate a degree of learning with chances better than random. However, due to the nature of the problem, we knew that results approaching 100% were possible if the agent was able to learn a consistent winning strategy. With a very large state space, limited discrete actions, and a relatively simple rewards structure, we modeled the game board for three RL agents to navigate against one random agent. 
We chose to compare the results of a SARSA, QL, and DQL agent on the metrics of average time steps taken for each game and win rate percentage. All 3 agents would reach above 99% win rates given certain hyperparameters and at various time steps. Overall the best consistently performing agent was QL, with a mean win rate of 93.62% and a standard variation of 10.09 across the different hyperparameters. The epsilon value had the greatest impact on the models’ performances, and a minuscule epsilon value of 0.01, alpha value of 0.1 or 0.2, and gamma value of 0.9 could guarantee the best performance among the 3 agents.
During our implementation, we had some discussions on how we wanted to model the environment, and how we would define a draw. Much like a human playing the game, we also needed to draw a balance of offense and defense, which would be modeled in the reward system. We also noticed that changing the number of rewards affected performance. For example, less aggressive rewards marginally made results worse. The data types and structures for how to store Q-values also changed a couple of times to make the program as efficient and clean as possible. We eventually decided to store the Q-table as a dictionary, with its key being a tuple representing each state and its value being the corresponding action values. This allows easy update and retrieval of the Q-values at each turn.
Future directions for the project could be to explore how introducing neural networks to create a Deep Q-Learning algorithm might improve performance. We could also develop a systematic way to compare the impacts of different hyperparameters since our current method of manually entering numbers wasn’t as efficient. It would also be helpful if we could show the game board and visualize the game process. Moreover, we are interested in exploring how to implement RL agents that could play this game against a human player, instead of a random agent. Instead of choosing their actions randomly without thinking, human players approach the game with their unique thoughtful strategies, therefore making it more difficult for the RL agents to consistently win the game.


